{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Follow the setup instructions to create the pipenv environment, then connect this notebook to the\n",
    "Python kernel in the \"`music-interpolation-...`\" environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Audio\n",
    "from music_interpolation.audio import load_audio, resample, to_mono_resampled, time_stretch, trim_samples_to_match\n",
    "from music_interpolation.encodec_interpolation import EncodecInterpolation\n",
    "from music_interpolation.beats import tempo_beats_downbeats\n",
    "\n",
    "AUDIO_A_PATH = \"../tests/data/house-equanimity.mp3\"\n",
    "AUDIO_B_PATH = \"../tests/data/they-know-me.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "interp = EncodecInterpolation(device=device)\n",
    "print(f\"Loaded {interp.sampling_rate} Hz interpolation model on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: basic\n",
    "\n",
    "# Load the audio files into raw waveform numpy arrays\n",
    "audio_a, orig_sr_a = load_audio(AUDIO_A_PATH)\n",
    "audio_b, orig_sr_b = load_audio(AUDIO_B_PATH)\n",
    "\n",
    "Audio(audio_a, rate=orig_sr_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_b, rate=orig_sr_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downmix to mono 44.1kHz for audio analysis\n",
    "audio_a_mono = to_mono_resampled(audio_a, orig_sr_a, 44100)\n",
    "audio_b_mono = to_mono_resampled(audio_b, orig_sr_b, 44100)\n",
    "\n",
    "audio_a_mono.shape, audio_b_mono.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tempo, confidence, beat positions, and beat positions (1-4) for each\n",
    "tempo_a, tempo_a_confidence, beats_downbeats_a = tempo_beats_downbeats(audio_a_mono)\n",
    "tempo_b, tempo_b_confidence, beats_downbeats_b = tempo_beats_downbeats(audio_b_mono)\n",
    "\n",
    "beats_downbeats_a.shape, beats_downbeats_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new arrays of bar positions for each audio file. beats_downbeats_a has shape\n",
    "# (num_beats, 2) where the first column is beat positions in seconds and the second\n",
    "# column is the bar index (1-4). We want to create a new array of shape (num_bars,)\n",
    "# where each element is the beat position of the first beat in the bar\n",
    "bars_a = beats_downbeats_a[beats_downbeats_a[:, 1] == 1, 0]\n",
    "bars_b = beats_downbeats_b[beats_downbeats_b[:, 1] == 1, 0]\n",
    "\n",
    "print(\n",
    "    f\"Tempo of audio_a: {tempo_a} bpm (confidence: {tempo_a_confidence * 100:.1f}%), \"\n",
    "    f\"{beats_downbeats_a.shape[0]} beats, {bars_a.shape[0]} bars\"\n",
    ")\n",
    "print(\n",
    "    f\"Tempo of audio_b: {tempo_b} bpm (confidence: {tempo_b_confidence * 100:.1f}%), \"\n",
    "    f\"{beats_downbeats_b.shape[0]} beats, {bars_b.shape[0]} bars\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually resample (if needed) instead of at load time to enable the highest\n",
    "# quality resampler\n",
    "if orig_sr_a != interp.sampling_rate:\n",
    "    audio_resampled_a = resample(audio_a, orig_sr_a, interp.sampling_rate)\n",
    "else:\n",
    "    audio_resampled_a = audio_a\n",
    "if orig_sr_b != interp.sampling_rate:\n",
    "    audio_resampled_b = resample(audio_b, orig_sr_b, interp.sampling_rate)\n",
    "else:\n",
    "    audio_resampled_b = audio_b\n",
    "\n",
    "audio_resampled_a.shape, audio_resampled_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time stretch track_b to match the tempo of track_a\n",
    "tempo_ratio = tempo_a / tempo_b\n",
    "if tempo_ratio != 1.0:\n",
    "    print(f\"Time stretching audio_b by {tempo_ratio:.3f}x\")\n",
    "    audio_stretched_b = time_stretch(audio_resampled_b, tempo_ratio)\n",
    "    bars_stretched_b = bars_b / tempo_ratio\n",
    "else:\n",
    "    audio_stretched_b = audio_resampled_b\n",
    "    bars_stretched_b = bars_b\n",
    "\n",
    "Audio(audio_stretched_b, rate=interp.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end of the interpolation in bars (1 bar = 4 beats)\n",
    "bar_start_a = 36\n",
    "bar_start_b = 12\n",
    "bar_count = 8\n",
    "\n",
    "# Calculate the start and end sample positions\n",
    "start_a = int(bars_a[bar_start_a] * interp.sampling_rate)\n",
    "start_b = int(bars_stretched_b[bar_start_b] * interp.sampling_rate)\n",
    "end_a = int(bars_a[bar_start_a + bar_count] * interp.sampling_rate)\n",
    "end_b = int(bars_stretched_b[bar_start_b + bar_count] * interp.sampling_rate)\n",
    "\n",
    "# Extract the audio for the interpolation\n",
    "audio_overlap_a = audio_resampled_a[:, start_a:end_a]\n",
    "audio_overlap_b = audio_stretched_b[:, start_b:end_b]\n",
    "\n",
    "print(f\"audio_overlap_a = {audio_overlap_a.shape[1] / interp.sampling_rate:.3f} seconds\")\n",
    "print(f\"audio_overlap_b = {audio_overlap_b.shape[1] / interp.sampling_rate:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim any rounding error frames so the samples exactly match\n",
    "audio_overlap_trimmed_a, audio_overlap_trimmed_b = trim_samples_to_match(audio_overlap_a, audio_overlap_b)\n",
    "\n",
    "print(audio_overlap_trimmed_a.shape, audio_overlap_trimmed_b.shape)\n",
    "Audio(audio_overlap_trimmed_a, rate=interp.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_overlap_trimmed_b, rate=interp.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract up to four bars of audio from audio_a leading up to the start of the\n",
    "# interpolation\n",
    "leadup_bars = 4\n",
    "leadup_samples_a = int(bars_a[bar_start_a - leadup_bars] * interp.sampling_rate)\n",
    "leadup_a = audio_resampled_a[:, leadup_samples_a:start_a]\n",
    "\n",
    "print(leadup_a.shape)\n",
    "Audio(leadup_a, rate=interp.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_c = interp.interpolate(audio_overlap_trimmed_a, audio_overlap_trimmed_b)\n",
    "\n",
    "Audio(audio_c, rate=interp.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models.musicgen import MusicGen\n",
    "\n",
    "print(f\"Loading MusicGen model ({device})\")\n",
    "model = MusicGen.get_pretrained(\"melody\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music_interpolation.musicgen import generate_continuation_with_chroma\n",
    "\n",
    "CFG_COEF = 3\n",
    "TEMPERATURE = 1\n",
    "\n",
    "leadup_duration = leadup_a.shape[1] / interp.sampling_rate\n",
    "overlap_duration = audio_c.shape[1] / interp.sampling_rate\n",
    "total_duration = leadup_duration + overlap_duration\n",
    "\n",
    "print(\n",
    "    f\"Generating {leadup_duration:.1f} + {overlap_duration:.1f} = \"\n",
    "    f\"{total_duration:.1f} seconds of audio\"\n",
    ")\n",
    "model.set_generation_params(duration=total_duration, cfg_coef=CFG_COEF, temperature=TEMPERATURE)\n",
    "prompt = torch.tensor(leadup_a)\n",
    "melody = torch.tensor(audio_c)\n",
    "melody_wavs = melody[None]\n",
    "wav = generate_continuation_with_chroma(\n",
    "    model, prompt, interp.sampling_rate, None, melody_wavs, interp.sampling_rate, progress=True\n",
    ")\n",
    "\n",
    "wav = wav[0].cpu().numpy()\n",
    "print(wav.shape)\n",
    "Audio(wav, rate=model.sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-interpolation-CiJIWn5K",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
